
<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style> 
		<title>PU-Dense: Sparse Tensor-based Point Cloud Geometry Upsampling</title>
  </head>

  <body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1049.0" data-gr-ext-installed="">
    <br>
          <center>
          	<span style="font-size:34px">PU-Dense: Sparse Tensor-based Point Cloud Geometry Upsampling</span><br>
	  		  
	  		  <br>

	  		  <table align="center" width="900px">
	  			  <tbody><tr>
	  	              <td align="center" width="250px">
	  					<center>
	  						<span style="font-size:22px"><a href="http://a.web.umkc.edu/aa95b/" target="_blank">Anique Akhtar</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px"><a href="http://l.web.umkc.edu/lizhu/" target="_blank">Zhu Li</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="300px">
	  					<center>
	  						<span style="font-size:22px"><a href="https://www.linkedin.com/in/geertvanderauwera/" target="_blank">Geert Van der Auwera</a><sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="100px">
	  					<center>
	  						<span style="font-size:22px"><a href="http://staff.ustc.edu.cn/~lilimao/" target="_blank">Li Li</a><sup>3</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="160px">
	  					<center>
	  						<span style="font-size:22px"><a href="https://www.linkedin.com/in/jianle-chen-63b9682b/" target="_blank">Jianle Chen</a><sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
	  			  </tr>
			  </tbody></table>
	  		  
			  <table align="center" width="800px"><tbody>
			  <tr>
	  	              <td align="center" width="50px"></td>
	  	              <td align="center" width="200px">
	  					<center>
				          	<span style="font-size:18px"><sup>1</sup>University of Missouri-Kansas City</span>
		  		  		</center>
		  	      </td>
	  	              <td align="center" width="150px">
	  					<center>
				          	<span style="font-size:18px"><sup>2</sup>Qualcomm Technologies Inc.</span>
		  		  		</center>
		  	      </td>
					  <td align="center" width="250px">
							<center>
								<span style="font-size:18px"><sup>3</sup>University of Science and Technology of China</span>
							</center>
					  </td>
	  	              <td align="center" width="50px"></td>
			  </tr>
			  </tbody></table>
			  

			  <br>

	  		  <table align="center" width="1100px">
	  			  <tbody><tr>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
	  						<span style="font-size:22px">Code <a href="https://github.com/aniqueakhtar/PointCloudUpsampling" target="_blank"> [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
							  <span style="font-size:22px">Unpublished<a href="" target="_blank"> [Paper]</a></span>
							  
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
			  </tr></tbody></table>
          </center>
<!--
          <br>
  		  <table align="center" width="1100px">
  			  <tbody><tr>
  	              <td width="400px">
  					<center>
  	                	<img class="rounded" src="./Figures/Meshes.png" width="1100px">
  	                	<br>
					</center>
  	              </td>
  	              </tr>
  	              </tbody></table>

  		  <br>
	  -->
		  <hr>

  		  <center><h1>Abstract</h1></center>
			Due to the increased popularity of augmented and virtual reality experiences, the interest in capturing high-resolution real-world point clouds has never been higher. 
	  		Loss of details and irregularities in point cloud geometry can occur during the capturing, processing, and compression pipeline. It is essential to address these challenges by being able to upsample a low Level-of-Detail (LoD) point cloud into a high LoD point cloud. Current upsampling methods suffer from several weaknesses in handling point cloud upsampling, especially in dense real-world photo-realistic point clouds. In this paper, we present a novel geometry upsampling technique, PU-Dense, which can process a diverse set of point clouds including synthetic mesh-based point clouds, real-world high-resolution point clouds, real-world indoor LiDAR scanned objects, as well as outdoor dynamically acquired LiDAR-based point clouds. PU-Dense employs a 3D multiscale architecture using sparse convolutional networks that hierarchically reconstruct an upsampled point cloud geometry via progressive rescaling and multiscale feature extraction. The framework employs a UNet type architecture that downscales the point cloud to a bottleneck and then upscales it to a higher level-of-detail (LoD) point cloud. PU-Dense introduces a novel Feature Extraction Unit that incorporates multiscale spatial learning by employing filters at multiple sampling rates and field of view. The architecture is memory efficient and is driven by a binary voxel occupancy classification loss that allows it to process high-resolution dense point clouds with millions of points during inference time. Qualitative and quantitative experimental results show that our method significantly outperforms the state-of-the-art approaches by a large margin while having much lower inference time complexity. We further test our dataset on high-resolution photo-realistic datasets. In addition, our method can handle noisy data well. We further show that our approach is memory efficient compared to the state-of-the-art methods.
		<br><br><hr>

		<center><h1>Overview</h1></center>
  		    <table align="center" width="1100px">
  			<tbody>
  			  	<tr>
  			  		<td align="center"><img class="round" style="width:1100px" src="./Figures/System.png"></td>	
			  	</tr>
			  	<!-- <tr>
			  		<td align="center"><h3>Pipeline of EC-Net</h3></td>
			  	</tr> -->
			</tbody>
			</table>
			
			<br>

			<!-- <table align="center" width="900px">
  			<tbody>
  			  	<tr>
  			  		<td align="center"><img class="round" style="width:700px" src="./figures/up&down.png"></td>	
			  	</tr>
			  	<tr>
			  		<td align="center"><h3>Patches Extraction</h3></td>
			  	</tr>

			</tbody>
			</table> -->
		  <br>

  		  <!-- <center>
				<span style="font-size:28px"><a href="https://github.com/liruihui/PU-GAN">[GitHub]</a>
			  <br>
			  </span></center>
		  <br> -->

<!--
	  
		  <hr>

  		  <center><h1>Paper and Supplementary Material</h1></center>
  		  <table align="center" width="500" px="">
	 		
  			  <tbody><tr>
				  <td><a href="https://liruihui.github.io/publication/PU-GAN/"><img class="layered-paper-big" style="height:175px" src="./PU-GAN_ a Point Cloud Upsampling Adversarial Network_files/paper.png"></a></td>
				  <td><span style="font-size:12pt">Ruihui Li, Xianzhi Li, Chi-Wing Fu, <br>Daniel Cohen-Or, Pheng-Ann Heng.</span><br>
				  <b><span style="font-size:12pt">PU-GAN: a Point Cloud Upsampling Adversarial Network.</span></b><br>
				  <span style="font-size:12pt">In ICCV, 2019. 
				  <br>
				  <a href="https://arxiv.org/abs/1907.10844" target="_blank">[arxiv]</a> 
				  <a href="https://drive.google.com/file/d/1CgOCgA62fmaVLBqK4SyEYa5LYoiZxtnN/view?usp=sharing" target="_blank">[paper]</a> 
				  <a href="https://drive.google.com/file/d/1e-pzQ4UFzeM54FWuwsfDcLGXuiyPuB9S/view?usp=sharing" target="_blank">[supp]</a>

				  </span></td>
  	              
              </tr>
  		  </tbody></table>
		  <br><br>
-->
		  <hr>

		<center><h1>Surface Reconstruction Results<br></h1></center>
		We show the visualized results of 3D surface reconstruction using the ball-pivoting algorithm for the ground truth, input point cloud, and the upsampled point clouds from 3PU, PU-GAN, PU-GCN, Dis-PU, and PU-Dense on <a href="https://shapenet.org/">ShapeNet dataset</a>. 
	  	In the 3D mesh reconstruction task, the result is greatly influenced by the density as well as the quality of the upsampled point cloud. 
	  	We can see the effectiveness of the proposed upsampling method by surface reconstruction in the figure below. 
	  	For both 4x and 8x upsampling, surfaces reconstructed from PU-Dense upsampled point clouds are a lot more structured and exhibit richer geometry details. 
	  	PU-Dense can recover more details and better preserve the smoothness of smooth regions as well as preserve the sharp edges without creating outliers.
  		<br><br>
  		<table align="center" width="1100px">
			  <tbody><tr>
				  <td><center>
				  	<img class="paper-big" style="width:1100px" src="./Figures/Meshes.jpg">
  	              </center></td>
              </tr>
  		</tbody></table> 
  		
  		<!-- <center>
  			<br>
  			<span style="font-size:28px"><a href="./moreresult.html">[More results]</a>
			<br>
			</span>
		</center> -->

		<br>
		<br>
		
		<hr>
 		<center><h1>Upsampling Results on High-Resolution Point Clouds<br></h1></center>
	  	  We show the visual results of our experiments for 4x and 8x upsampling in the figures below using a point cloud frame from sequence <i>RedandBlack</i> and <i>Longdress</i> respectively from <a href="http://plenodb.jpeg.org/pc/8ilabs/">8iVFB dataset</a>.
	  	  This is a real-world captured dense photo-realistic dynamic point cloud dataset for immersive communication used in <a href="https://mpeg-pcc.org/index.php/pcc-content-database/8i-voxelized-surface-light-field-8ivslf-dataset/">MPEG</a> and <a href="https://plenodb.jpeg.org/">JPEG</a> point cloud compression standardization.
	  	  We also plot the error map based on the point-to-point (P2point) D1 distance between the point cloud and its ground truth to visualize the error distribution. 
	  	  We can see that our method generates limited outliers while populating points very close to the surface of the ground truth point cloud. 
	  	  This results in a high-resolution point cloud with considerably better quality than the other state-of-the-arts. 
	      We also show the zoomed-in point cloud geometry views. The results of our proposed method are more precise, sharper, and cleaner, especially around key positions, such as corners and edges. 
  		<br><br>
  		<table align="center" width="800px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:900px" src="./Figures/redandblack_4x.jpg">
  	              </center></span></td>
			  </tr>
			  <tr>
			  	<td align="center">4x upsampling visual results on sequence <i>RedandBlack</i>.</td>
			  </tr>
  		</tbody></table> 
  		<br>
  		<table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:900px" src="./Figures/longdress_8x.jpg">
  	              </center></span></td>
			  </tr>
			  <tr>
				<td align="center">8x upsampling visual results on sequence <i>Longdress</i></td>
			  </tr>
  		</tbody></table> 
  		
  		<br>
		  
		<hr>

		<center><h1>Results on Real Scanned Object Dataset<br></h1></center>
	     We also examined the performance of the proposed method on real-world scanned object dataset with 2048 points from <a href="https://hkust-vgd.github.io/scanobjectnn/">ScanObjectNN dataset</a>. 
	     We show the comparative visual results of upsampling a point cloud from ScanObjectNN dataset in the figure below. ScanObjectNN dataset is captured through LiDAR and consists of sparse point clouds. 
	  	 We visualize the upsampled point cloud as well as the reconstructed mesh surface using the ball-pivoting algorithm for the input point cloud and each of the upsampled point clouds. 
	  	 The proposed method (PU-Dense) works well on even sparse object point cloud datasets. In the upsampled point cloud generated by PU-Dense, the points generated are a lot more structured. 
	     We can notice that the holes in the point cloud get impainted by the other methods, whereas, PU-Dense keeps the original shape of the point cloud. 
	     The reconstructed mesh surface quality for PU-dense is also better compared with 3PU and PU-GAN. 

  		<br><br>
  		<table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:1000px" src="./Figures/ScanObjectNN.jpg">
  	              </center></span></td>
              </tr>
			  <tr>
				<td align="center">Visual comparison for 4x upsampling on ScanObjectNN dataset</td>
			  </tr>
  		</tbody></table> 

  		<br>
		  <br>
	  
	  
	  <hr>

		<center><h1>Results on Dynamically Acquired Outdoor Dataset<br></h1></center>
	  
	  	We examine the performance of PU-Dense on dynamically-acquired outdoor LiDAR dataset from <a href="http://www.cvlibs.net/datasets/kitti/">KITTI dataset</a> used for autonomous driving. 
	  We show the visual results from four street point clouds in the figure below. Even for sparse and non-uniform point clouds from real-world LiDAR dataset, our proposed method can significantly improve the quality by upsampling these point clouds. 
	  In our experiments, we trained PU-Dense on regularly sampled and relatively dense ShapeNet dataset. Therefore, it is a considerable achievement for PU-Dense to be able to generalize to a sparse non-uniform dataset. 
	  
  		<br><br>
  		<table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:1000px" src="./Figures/Kitti.png">
  	              </center></span></td>
              </tr>
			  <tr>
			  	<td align="center">Visual results for outdoor LiDAR dataset from KITTI for 4x upsampling.</td>
			  </tr>
  		</tbody></table> 

  		<br>
		  <br>
	  
	  <hr>

		<center><h1>Robustness against Noise<br></h1></center>
		  In this section, we evaluate the robustness of different methods to noise. The plot below quantitatively compares the methods on different levels of noise using <a href="https://shapenet.org/">ShapeNet dataset</a>  for 4x upsampling. 
	  We plot D1 MSE PSNR for each method under 7 levels of Gaussian noise. It can be seen that the performance of all methods decreases as the noise level increase. 
	  Nevertheless, the proposed method consistently achieves the best performance under each noise level. 
	  For PU-Dense, we notice the sharpest decrease when the noise is initially added and then the quality consistently decreases with an increase in noise. 
	  The robustness against noise can be further increased by augmenting noise during training of PU-Dense, something that was not employed during our training. 
	  We further show visual upsampling results for the PU-Dense on various noisy point clouds in the figure below. 
	  We visualize both 4x and 8x upsampling with 0%, 1%, and 2% Gaussian noise. 
	  We observe that even after adding noise, the upsampled point clouds visually look similar to the noise-free upsampled point cloud, demonstrating the robustness of the proposed method against noise.

  		<br><br>
  		<table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:700px" src="./Figures/noise_plot.jpg">
  	              </center></span></td>
			  </tr>
			  <tr>
			  	<td align="center">Quantitative comparisons on data with various levels of noise for 4x upsampling using ShapeNet dataset.</td>
			  </tr>
  		</tbody></table> 
  		<br>
  		<table align="center" width="1100px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<img class="paper-big" style="width:1100px" src="./Figures/Noise.png">
  	              </center></span></td>
			  </tr>
			  <tr>
				<td align="center">Visual results for PU-Dense on noisy data for 4x and 8x upsampling. Left: (a1), (b1), and (c1) are the sparse inputs with 0%, 1%, and 2% Gaussian
noise, respectively. Right: (a2), (b2), and (c2) are the upsampled results from (a1), (b1), and (c1) respectively.</td>
			  </tr>
  		</tbody></table> 
  		
  		<br>
  		


		<hr>		
  		  <table align="center" width="1100px">
  			<tbody>
			<tr><td width="400px"><left>
			  <center><h1>Citation</h1></center>
			  Paper accepted in IEEE Transactions on Image Processing. Citation available soon.
				<br>
			  If PU-Dense is useful for your research, please consider citing.

<!--		
				
			<p style="text-align:left;">
				<span>@inproceedings{pudense,</span><br>
				<span>  title&nbsp;=&nbsp;{<span id="__kindeditor_bookmark_start_4__"></span>{PU-Dense}: Sparse Tensor-based Point Cloud Geometry Upsampling<span id="__kindeditor_bookmark_end_5__"></span>},</span><br>
				<span>  author&nbsp;=&nbsp;{Akhtar, Anique and Li, Zhu and Van der Auwera, Geert and Li, Li and Chen, Jianle},</span><br>
				<span>  booktitle&nbsp;=&nbsp;{{IEEE} Transactions on Image Processing.},</span><br>
				<span>  year&nbsp;=&nbsp;{2022},</span><br>
				<span>}&nbsp;</span> 
			</p>
			</left></td></tr>
			</tbody></table>
		<br>
		<br>

	  
		<hr>
  		  <table align="center" width="1100px">
  			<tbody>
			<tr><td width="400px"><left>
	  		<center><h1>Acknowledgments</h1></center>
		          We thank anonymous reviewers for the valuable comments.
			We also thank <a href="https://wbstx.github.io/about/" target="_blank"> xiao tang</a>
			 for collecting the dataset and hao xu
                         for rendering the vivid figures. 
			 The work is supported by the 973 Program (Proj. No. 2015CB351706), the National Natural Science Foundation of China with Proj. No. U1613219, the
			  Research Grants Council of the Hong Kong Special Administrative Region (No. CUHK 14203416 &amp; 14201717), and
			  the Israel Science Foundation grants 2366/16 and 2472/7.
			</left></td></tr>
			</tbody></table>
		<br>
		<br>
	
	  

-->
	  
<br>
<br>
<br>
<br>
<br>
<br><br>
<br>
<br>
<br>
<br>
<br>

	  
<!-- Default Statcounter code for PU-Dense
https://aniqueakhtar.github.io/publications/PU-Dense/ -->
<script type="text/javascript">
var sc_project=12724534; 
var sc_invisible=1; 
var sc_security="c4165498"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics
Made Easy - Statcounter" href="https://statcounter.com/"
target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12724534/0/c4165498/1/"
alt="Web Analytics Made Easy - Statcounter"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->